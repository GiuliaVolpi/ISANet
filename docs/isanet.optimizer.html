

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>isanet.optimizer &mdash; IsaNet ML Lib 0.1 beta documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="isanet.utils" href="isanet.utils.html" />
    <link rel="prev" title="isanet.neural_network" href="isanet.neural_network.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> IsaNet ML Lib
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Modules:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="isanet.activation.html">isanet.activation</a></li>
<li class="toctree-l1"><a class="reference internal" href="isanet.datasets.html">isanet.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="isanet.metrics.html">isanet.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="isanet.model.html">isanet.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="isanet.model_selection.html">isanet.model_selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="isanet.neural_network.html">isanet.neural_network</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">isanet.optimizer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-isanet.optimizer.optimizer">isanet.optimizer.optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-isanet.optimizer.SGD">isanet.optimizer.SGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-isanet.optimizer.NCG">isanet.optimizer.NCG</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-isanet.optimizer.LBFGS">isanet.optimizer.LBFGS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-isanet.optimizer.linesearch">isanet.optimizer.linesearch</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-isanet.optimizer.utils">isanet.optimizer.utils</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="isanet.utils.html">isanet.utils</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">IsaNet ML Lib</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>isanet.optimizer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/isanet.optimizer.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="isanet-optimizer">
<h1>isanet.optimizer<a class="headerlink" href="#isanet-optimizer" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-isanet.optimizer.optimizer">
<span id="isanet-optimizer-optimizer"></span><h2>isanet.optimizer.optimizer<a class="headerlink" href="#module-isanet.optimizer.optimizer" title="Permalink to this headline">¶</a></h2>
<p>Optimizer Module.</p>
<dl class="py class">
<dt id="isanet.optimizer.optimizer.EarlyStopping">
<em class="property">class </em><code class="sig-prename descclassname">isanet.optimizer.optimizer.</code><code class="sig-name descname">EarlyStopping</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">eps</span></em>, <em class="sig-param"><span class="n">patience</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.EarlyStopping" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stop training when a the MSE (Mean squared error) on validation
(generalization error) is increasing and exceeds a certain threshold
for a finite number of epochs.</p>
<p class="rubric">Notes</p>
<p>The class define the generalization loss at epoch t to be the relative
increase of the validation error over the minimum-so-far (in percent):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GL</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">mse_val</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">/</span><span class="n">min_mse_val</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Then it will stop the optimization when the generatilization loss exceeds
a certain thresold for a finite number of epochs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">G</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eps</strong> (<em>float</em>) – Threshold that is used to decide whether to stop the
fitting of the model (it stops if this is true and after
a number of epoch &gt; ‘patience’).</p></li>
<li><p><strong>patience</strong> (<em>integer</em>) – Number of epochs that mse should be worse after which training
will be stopped.</p></li>
<li><p><strong>verbose</strong> (<em>boolean</em><em>, </em><em>default=False</em>) – Whether to print progress messages to stdout.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="isanet.optimizer.optimizer.EarlyStopping.check_early_stop">
<code class="sig-name descname">check_early_stop</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">epoch</span></em>, <em class="sig-param"><span class="n">history</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.EarlyStopping.check_early_stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if Early Stopping criteria has occurred.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>isanet.model.MLP</em>) – </p></li>
<li><p><strong>epoch</strong> (<em>integer</em>) – Current number of epoch (epoch == 0 is the first epoch).</p></li>
<li><p><strong>history</strong> (<em>dict</em>) – Contains, for the current epoch, the values of mse, mee and accuracy
for training and validation and the time taken to compute that epoch.
This parameter is used to check the value of current MSE (Mean squared
error) on validation (generalization error).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if the Early stopping has occurred, else False</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>boolean</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.EarlyStopping.get_min_val">
<code class="sig-name descname">get_min_val</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.EarlyStopping.get_min_val" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the min mse on validation if a minimum of the generalization
has been reached after overfitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Min of the mse on validation.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.EarlyStopping.get_weights_backup">
<code class="sig-name descname">get_weights_backup</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.EarlyStopping.get_weights_backup" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the weights’s backup if a minimum of the generalization
has been reached after overfitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Weights’s backup.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list of arrays-like</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="isanet.optimizer.optimizer.Optimizer">
<em class="property">class </em><code class="sig-prename descclassname">isanet.optimizer.optimizer.</code><code class="sig-name descname">Optimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_iter_no_change</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">norm_g_eps</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">l_eps</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">debug</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.backpropagation">
<code class="sig-name descname">backpropagation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">weights</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.backpropagation" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the derivative of 1/2 sum_n (y_i -y_i’)
:param model: Specify the Multilayer Perceptron object to optimize
:type model: isanet.model.MLP
:param X: The input data.
:type X: array-like of shape (n_samples, n_features)
:param Y: The target values.
:type Y: array-like of shape (n_samples, n_output)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>contains the gradients for each layer to be used in the delta rule.
Each key in the dictionary represents the ith layer. (from the first
hidden layer to the output layer).:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">E</span><span class="o">.</span><span class="n">g</span><span class="o">.</span> <span class="mi">0</span> <span class="o">-&gt;</span> <span class="n">first</span> <span class="n">hidden</span> <span class="n">layer</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">output</span> <span class="n">layer</span>
<span class="n">where</span> <span class="n">n</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">hidden</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">net</span><span class="o">.</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span></em>, <em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.get_batch">
<code class="sig-name descname">get_batch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X_train</span></em>, <em class="sig-param"><span class="n">Y_train</span></em>, <em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.get_batch" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p></li>
<li><p><strong>Y_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_output</em><em>)</em>) – The target values.</p></li>
<li><p><strong>batch_size</strong> (<em>integer</em>) – Size of minibatches for the optimizer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Each key of the dictionary is a integer value from 0 to
number_of_batch -1 and define a batch. Each element is a
dictionary and has two key: ‘batch_x_train’ and ‘batch_y_train’
and refer to the portion of data and target respectively used
for the training.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict of dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.get_epoch_history">
<code class="sig-name descname">get_epoch_history</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">X_train</span></em>, <em class="sig-param"><span class="n">Y_train</span></em>, <em class="sig-param"><span class="n">validation_data</span></em>, <em class="sig-param"><span class="n">time</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.get_epoch_history" title="Permalink to this definition">¶</a></dt>
<dd><p>Given the model, training data, validation data and time returns a dictionary
that contains:</p>
<blockquote>
<div><dl class="simple">
<dt>{“mse_train”: mse_train,</dt><dd><p>“mee_train”: mee_train,
“acc_train”: acc_train,
“mse_val”: mse_val,
“mee_val”: mee_val,
“acc_val”: acc_val,
“time”: time</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.optimize">
<code class="sig-name descname">optimize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">epochs</span></em>, <em class="sig-param"><span class="n">X_train</span></em>, <em class="sig-param"><span class="n">Y_train</span></em>, <em class="sig-param"><span class="n">validation_data</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">es</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.optimize" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>isanet.model.MLP</em>) – Specify the Multilayer Perceptron object to optimize</p></li>
<li><p><strong>epochs</strong> (<em>integer</em>) – Maximum number of epochs.</p></li>
<li><p><strong>X_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p></li>
<li><p><strong>Y_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_output</em><em>)</em>) – The target values.</p></li>
<li><p><strong>validation_data</strong> (<em>list of arrays-like</em><em>, </em><em>[</em><em>X_val</em><em>, </em><em>Y_val</em><em>]</em><em>, </em><em>optional</em>) – Validation set.</p></li>
<li><p><strong>batch_size</strong> (<em>integer</em><em>, </em><em>optional</em>) – Size of minibatches for the optimizer.
When set to “none”, the optimizer will performe a full batch.</p></li>
<li><p><strong>es</strong> (<em>isanet.callbacks.EarlyStopping</em><em>, </em><em>optional</em>) – When set to None it will only use the <code class="docutils literal notranslate"><span class="pre">epochs</span></code> to finish training.
Otherwise, an EarlyStopping type object has been passed and will stop
training if the model goes overfitting after a number of consecutive iterations.
See docs in optimizier module for the EarlyStopping Class.</p></li>
<li><p><strong>verbose</strong> (<em>integer</em><em>, </em><em>default=0</em>) – Controls the verbosity: the higher, the more messages.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>integer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span></em>, <em class="sig-param"><span class="n">verbose</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-isanet.optimizer.SGD">
<span id="isanet-optimizer-sgd"></span><h2>isanet.optimizer.SGD<a class="headerlink" href="#module-isanet.optimizer.SGD" title="Permalink to this headline">¶</a></h2>
<p>Optimizer Module.</p>
<dl class="py class">
<dt id="isanet.optimizer.SGD.SGD">
<em class="property">class </em><code class="sig-prename descclassname">isanet.optimizer.SGD.</code><code class="sig-name descname">SGD</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">lr</span><span class="o">=</span><span class="default_value">0.1</span></em>, <em class="sig-param"><span class="n">momentum</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">nesterov</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">sigma</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_iter_no_change</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">norm_g_eps</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">l_eps</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">debug</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.SGD.SGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#isanet.optimizer.optimizer.Optimizer" title="isanet.optimizer.optimizer.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">isanet.optimizer.optimizer.Optimizer</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – Learning rate schedule for weight updates (delta rule).</p></li>
<li><p><strong>momentum</strong> (<em>float</em><em>, </em><em>default=0</em>) – Momentum for gradient descent update.</p></li>
<li><p><strong>nesterov</strong> (<em>boolean</em><em>, </em><em>default=False</em>) – Whether to use Nesterov’s momentum.</p></li>
<li><p><strong>sigma</strong> (<em>float</em><em>, </em><em>default=None</em>) – Parameter of the Super Accelerated Nesterov’s momentum.
If ‘nesterov’ is True and ‘sigma’ equals to ‘momentum’, then we have the
simple Nesterov momentum. Instead, if ‘sigma’ is different from
‘momentum’, we have the super accelerated Nesterov.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=None</em>) – Tolerance for the optimization. When the loss on training is
not improving by at least tol for ‘n_iter_no_change’ consecutive
iterations convergence is considered to be reached and training stops.</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>integer</em><em>, </em><em>default=None</em>) – Maximum number of epochs with no improvements &gt; tol.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="isanet.optimizer.SGD.SGD.optimize">
<code class="sig-name descname">optimize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">epochs</span></em>, <em class="sig-param"><span class="n">X_train</span></em>, <em class="sig-param"><span class="n">Y_train</span></em>, <em class="sig-param"><span class="n">validation_data</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">es</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.SGD.SGD.optimize" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>isanet.model.MLP</em>) – Specify the Multilayer Perceptron object to optimize</p></li>
<li><p><strong>epochs</strong> (<em>integer</em>) – Maximum number of epochs.</p></li>
<li><p><strong>X_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p></li>
<li><p><strong>Y_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_output</em><em>)</em>) – The target values.</p></li>
<li><p><strong>validation_data</strong> (<em>list of arrays-like</em><em>, </em><em>[</em><em>X_val</em><em>, </em><em>Y_val</em><em>]</em><em>, </em><em>optional</em>) – Validation set.</p></li>
<li><p><strong>batch_size</strong> (<em>integer</em><em>, </em><em>optional</em>) – Size of minibatches for the optimizer.
When set to “none”, the optimizer will performe a full batch.</p></li>
<li><p><strong>es</strong> (<em>isanet.callbacks.EarlyStopping</em><em>, </em><em>optional</em>) – When set to None it will only use the <code class="docutils literal notranslate"><span class="pre">epochs</span></code> to finish training.
Otherwise, an EarlyStopping type object has been passed and will stop
training if the model goes overfitting after a number of consecutive iterations.
See docs in optimizier module for the EarlyStopping Class.</p></li>
<li><p><strong>verbose</strong> (<em>integer</em><em>, </em><em>default=0</em>) – Controls the verbosity: the higher, the more messages.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>integer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.SGD.SGD.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span></em>, <em class="sig-param"><span class="n">verbose</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.SGD.SGD.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-isanet.optimizer.NCG">
<span id="isanet-optimizer-ncg"></span><h2>isanet.optimizer.NCG<a class="headerlink" href="#module-isanet.optimizer.NCG" title="Permalink to this headline">¶</a></h2>
<p>Optimizer Module.</p>
<dl class="py class">
<dt id="isanet.optimizer.NCG.NCG">
<em class="property">class </em><code class="sig-prename descclassname">isanet.optimizer.NCG.</code><code class="sig-name descname">NCG</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">beta_method</span><span class="o">=</span><span class="default_value">'hs+'</span></em>, <em class="sig-param"><span class="n">c1</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">c2</span><span class="o">=</span><span class="default_value">0.9</span></em>, <em class="sig-param"><span class="n">restart</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">sfgrd</span><span class="o">=</span><span class="default_value">0.01</span></em>, <em class="sig-param"><span class="n">ln_maxiter</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_iter_no_change</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">norm_g_eps</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">l_eps</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">debug</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#isanet.optimizer.optimizer.Optimizer" title="isanet.optimizer.optimizer.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">isanet.optimizer.optimizer.Optimizer</span></code></a></p>
<dl class="py method">
<dt id="isanet.optimizer.NCG.NCG.append_history">
<code class="sig-name descname">append_history</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">beta</span></em>, <em class="sig-param"><span class="n">alpha</span></em>, <em class="sig-param"><span class="n">norm_g</span></em>, <em class="sig-param"><span class="n">ls_log</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG.append_history" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.NCG.NCG.backpropagation">
<code class="sig-name descname">backpropagation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">weights</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG.backpropagation" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the derivative of 1/2 sum_n (y_i -y_i’)
:param model: Specify the Multilayer Perceptron object to optimize
:type model: isanet.model.MLP
:param X: The input data.
:type X: array-like of shape (n_samples, n_features)
:param Y: The target values.
:type Y: array-like of shape (n_samples, n_output)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>contains the gradients for each layer to be used in the delta rule.
Each key in the dictionary represents the ith layer. (from the first
hidden layer to the output layer).:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">E</span><span class="o">.</span><span class="n">g</span><span class="o">.</span> <span class="mi">0</span> <span class="o">-&gt;</span> <span class="n">first</span> <span class="n">hidden</span> <span class="n">layer</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">output</span> <span class="n">layer</span>
<span class="n">where</span> <span class="n">n</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">hidden</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">net</span><span class="o">.</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.NCG.NCG.beta_fr">
<code class="sig-name descname">beta_fr</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">past_g</span></em>, <em class="sig-param"><span class="n">past_norm_g</span></em>, <em class="sig-param"><span class="n">past_d</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG.beta_fr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.NCG.NCG.beta_hs">
<code class="sig-name descname">beta_hs</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">past_g</span></em>, <em class="sig-param"><span class="n">past_norm_g</span></em>, <em class="sig-param"><span class="n">past_d</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG.beta_hs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.NCG.NCG.beta_hs_plus">
<code class="sig-name descname">beta_hs_plus</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">past_g</span></em>, <em class="sig-param"><span class="n">past_norm_g</span></em>, <em class="sig-param"><span class="n">past_d</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG.beta_hs_plus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.NCG.NCG.beta_pr">
<code class="sig-name descname">beta_pr</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">past_g</span></em>, <em class="sig-param"><span class="n">past_norm_g</span></em>, <em class="sig-param"><span class="n">past_d</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG.beta_pr" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.NCG.NCG.beta_pr_plus">
<code class="sig-name descname">beta_pr_plus</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">past_g</span></em>, <em class="sig-param"><span class="n">past_norm_g</span></em>, <em class="sig-param"><span class="n">past_d</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG.beta_pr_plus" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.NCG.NCG.get_beta_function">
<code class="sig-name descname">get_beta_function</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">beta_method</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG.get_beta_function" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.NCG.NCG.optimize">
<code class="sig-name descname">optimize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">epochs</span></em>, <em class="sig-param"><span class="n">X_train</span></em>, <em class="sig-param"><span class="n">Y_train</span></em>, <em class="sig-param"><span class="n">validation_data</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">es</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG.optimize" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>isanet.model.MLP</em>) – Specify the Multilayer Perceptron object to optimize</p></li>
<li><p><strong>epochs</strong> (<em>integer</em>) – Maximum number of epochs.</p></li>
<li><p><strong>X_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p></li>
<li><p><strong>Y_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_output</em><em>)</em>) – The target values.</p></li>
<li><p><strong>validation_data</strong> (<em>list of arrays-like</em><em>, </em><em>[</em><em>X_val</em><em>, </em><em>Y_val</em><em>]</em><em>, </em><em>optional</em>) – Validation set.</p></li>
<li><p><strong>batch_size</strong> (<em>integer</em><em>, </em><em>optional</em>) – Size of minibatches for the optimizer.
When set to “none”, the optimizer will performe a full batch.</p></li>
<li><p><strong>es</strong> (<em>isanet.callbacks.EarlyStopping</em><em>, </em><em>optional</em>) – When set to None it will only use the <code class="docutils literal notranslate"><span class="pre">epochs</span></code> to finish training.
Otherwise, an EarlyStopping type object has been passed and will stop
training if the model goes overfitting after a number of consecutive iterations.
See docs in optimizier module for the EarlyStopping Class.</p></li>
<li><p><strong>verbose</strong> (<em>integer</em><em>, </em><em>default=0</em>) – Controls the verbosity: the higher, the more messages.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>integer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.NCG.NCG.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span></em>, <em class="sig-param"><span class="n">verbose</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.NCG.NCG.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-isanet.optimizer.LBFGS">
<span id="isanet-optimizer-lbfgs"></span><h2>isanet.optimizer.LBFGS<a class="headerlink" href="#module-isanet.optimizer.LBFGS" title="Permalink to this headline">¶</a></h2>
<p>Optimizer Module.</p>
<dl class="py class">
<dt id="isanet.optimizer.LBFGS.LBFGS">
<em class="property">class </em><code class="sig-prename descclassname">isanet.optimizer.LBFGS.</code><code class="sig-name descname">LBFGS</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">m</span><span class="o">=</span><span class="default_value">3</span></em>, <em class="sig-param"><span class="n">c1</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">c2</span><span class="o">=</span><span class="default_value">0.9</span></em>, <em class="sig-param"><span class="n">ln_maxiter</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_iter_no_change</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">norm_g_eps</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">l_eps</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">debug</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.LBFGS.LBFGS" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#isanet.optimizer.optimizer.Optimizer" title="isanet.optimizer.optimizer.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">isanet.optimizer.optimizer.Optimizer</span></code></a></p>
<dl class="py method">
<dt id="isanet.optimizer.LBFGS.LBFGS.append_history">
<code class="sig-name descname">append_history</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">alpha</span></em>, <em class="sig-param"><span class="n">norm_g</span></em>, <em class="sig-param"><span class="n">ls_log</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.LBFGS.LBFGS.append_history" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.LBFGS.LBFGS.backpropagation">
<code class="sig-name descname">backpropagation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">weights</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.LBFGS.LBFGS.backpropagation" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the derivative of 1/2 sum_n (y_i -y_i’)
:param model: Specify the Multilayer Perceptron object to optimize
:type model: isanet.model.MLP
:param X: The input data.
:type X: array-like of shape (n_samples, n_features)
:param Y: The target values.
:type Y: array-like of shape (n_samples, n_output)</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>contains the gradients for each layer to be used in the delta rule.
Each key in the dictionary represents the ith layer. (from the first
hidden layer to the output layer).:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">E</span><span class="o">.</span><span class="n">g</span><span class="o">.</span> <span class="mi">0</span> <span class="o">-&gt;</span> <span class="n">first</span> <span class="n">hidden</span> <span class="n">layer</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">output</span> <span class="n">layer</span>
<span class="n">where</span> <span class="n">n</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">hidden</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">net</span><span class="o">.</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.LBFGS.LBFGS.compute_search_dir">
<code class="sig-name descname">compute_search_dir</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">g</span></em>, <em class="sig-param"><span class="n">H0</span></em>, <em class="sig-param"><span class="n">s</span></em>, <em class="sig-param"><span class="n">y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.LBFGS.LBFGS.compute_search_dir" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.LBFGS.LBFGS.optimize">
<code class="sig-name descname">optimize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">epochs</span></em>, <em class="sig-param"><span class="n">X_train</span></em>, <em class="sig-param"><span class="n">Y_train</span></em>, <em class="sig-param"><span class="n">validation_data</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">es</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.LBFGS.LBFGS.optimize" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>isanet.model.MLP</em>) – Specify the Multilayer Perceptron object to optimize</p></li>
<li><p><strong>epochs</strong> (<em>integer</em>) – Maximum number of epochs.</p></li>
<li><p><strong>X_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p></li>
<li><p><strong>Y_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_output</em><em>)</em>) – The target values.</p></li>
<li><p><strong>validation_data</strong> (<em>list of arrays-like</em><em>, </em><em>[</em><em>X_val</em><em>, </em><em>Y_val</em><em>]</em><em>, </em><em>optional</em>) – Validation set.</p></li>
<li><p><strong>batch_size</strong> (<em>integer</em><em>, </em><em>optional</em>) – Size of minibatches for the optimizer.
When set to “none”, the optimizer will performe a full batch.</p></li>
<li><p><strong>es</strong> (<em>isanet.callbacks.EarlyStopping</em><em>, </em><em>optional</em>) – When set to None it will only use the <code class="docutils literal notranslate"><span class="pre">epochs</span></code> to finish training.
Otherwise, an EarlyStopping type object has been passed and will stop
training if the model goes overfitting after a number of consecutive iterations.
See docs in optimizier module for the EarlyStopping Class.</p></li>
<li><p><strong>verbose</strong> (<em>integer</em><em>, </em><em>default=0</em>) – Controls the verbosity: the higher, the more messages.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>integer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.LBFGS.LBFGS.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span></em>, <em class="sig-param"><span class="n">verbose</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.LBFGS.LBFGS.step" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-isanet.optimizer.linesearch">
<span id="isanet-optimizer-linesearch"></span><h2>isanet.optimizer.linesearch<a class="headerlink" href="#module-isanet.optimizer.linesearch" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="isanet.optimizer.linesearch.LineSearch">
<em class="property">class </em><code class="sig-prename descclassname">isanet.optimizer.linesearch.</code><code class="sig-name descname">LineSearch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">phi</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.linesearch.LineSearch" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt id="isanet.optimizer.linesearch.LineSearch.set_phi">
<code class="sig-name descname">set_phi</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.linesearch.LineSearch.set_phi" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.linesearch.LineSearch.strong_wolfe">
<code class="sig-name descname">strong_wolfe</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">phi0</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">old_phi0</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">derphi0</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">c1</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">c2</span><span class="o">=</span><span class="default_value">0.9</span></em>, <em class="sig-param"><span class="n">amax</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">maxiter</span><span class="o">=</span><span class="default_value">10</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.linesearch.LineSearch.strong_wolfe" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="isanet.optimizer.linesearch.line_search_wolfe">
<code class="sig-prename descclassname">isanet.optimizer.linesearch.</code><code class="sig-name descname">line_search_wolfe</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">phi</span></em>, <em class="sig-param"><span class="n">derphi</span></em>, <em class="sig-param"><span class="n">phi0</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">old_phi0</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">derphi0</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">c1</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">c2</span><span class="o">=</span><span class="default_value">0.9</span></em>, <em class="sig-param"><span class="n">amax</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">maxiter</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.linesearch.line_search_wolfe" title="Permalink to this definition">¶</a></dt>
<dd><p>Return alpha &gt; 0 that satisfies strong Wolfe conditions in order
to get a descent direction or the last alpha found if the line search
algorithm did not converge.
For major details on the implementation refer to Wright and Nocedal,
‘Numerical Optimization’, 1999, pp. 59-61.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>phi</strong> (<em>callable phi</em><em>(</em><em>alpha</em><em>)</em>) – Objective scalar function.</p></li>
<li><p><strong>derphi</strong> (<em>callable phi'</em><em>(</em><em>alpha</em><em>)</em>) – Objective function derivative. Returns a scalar.</p></li>
<li><p><strong>phi0</strong> (<em>float</em><em>, </em><em>optional</em>) – Value of phi at 0.</p></li>
<li><p><strong>old_phi0</strong> (<em>float</em><em>, </em><em>optional</em>) – Value of phi at previous point.</p></li>
<li><p><strong>derphi0</strong> (<em>float</em><em>, </em><em>optional</em>) – Value of derphi at 0</p></li>
<li><p><strong>c1</strong> (<em>float</em><em>, </em><em>optional</em>) – Parameter for Armijo condition rule.</p></li>
<li><p><strong>c2</strong> (<em>float</em><em>, </em><em>optional</em>) – Parameter for curvature condition rule.</p></li>
<li><p><strong>amax</strong> (<em>float</em><em>, </em><em>optional</em>) – Maximum step size.</p></li>
<li><p><strong>maxiter</strong> (<em>int</em><em>, </em><em>optional</em>) – Maximum number of iterations to perform.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>alpha_star</strong> – Best alpha, or last alpha if the line search algorithm did not converge.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="isanet.optimizer.linesearch.line_search_wolfe_f">
<code class="sig-prename descclassname">isanet.optimizer.linesearch.</code><code class="sig-name descname">line_search_wolfe_f</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">phi</span></em>, <em class="sig-param"><span class="n">derphi</span></em>, <em class="sig-param"><span class="n">phi0</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">c1</span><span class="o">=</span><span class="default_value">0.0001</span></em>, <em class="sig-param"><span class="n">c2</span><span class="o">=</span><span class="default_value">0.9</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.linesearch.line_search_wolfe_f" title="Permalink to this definition">¶</a></dt>
<dd><p>a fast strong Wolfe line search</p>
</dd></dl>

<dl class="py class">
<dt id="isanet.optimizer.linesearch.phi_function">
<em class="property">class </em><code class="sig-prename descclassname">isanet.optimizer.linesearch.</code><code class="sig-name descname">phi_function</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">optimizer</span></em>, <em class="sig-param"><span class="n">w</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span></em>, <em class="sig-param"><span class="n">d</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.linesearch.phi_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt id="isanet.optimizer.linesearch.phi_function.derphi">
<code class="sig-name descname">derphi</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.linesearch.phi_function.derphi" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.linesearch.phi_function.phi">
<code class="sig-name descname">phi</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.linesearch.phi_function.phi" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-isanet.optimizer.utils">
<span id="isanet-optimizer-utils"></span><h2>isanet.optimizer.utils<a class="headerlink" href="#module-isanet.optimizer.utils" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="isanet.optimizer.utils.l_norm">
<code class="sig-prename descclassname">isanet.optimizer.utils.</code><code class="sig-name descname">l_norm</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">l_v</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.utils.l_norm" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="isanet.optimizer.utils.l_scalar_product">
<code class="sig-prename descclassname">isanet.optimizer.utils.</code><code class="sig-name descname">l_scalar_product</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">l_v</span></em>, <em class="sig-param"><span class="n">l_w</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.utils.l_scalar_product" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="isanet.optimizer.utils.make_vector">
<code class="sig-prename descclassname">isanet.optimizer.utils.</code><code class="sig-name descname">make_vector</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">l_v</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.utils.make_vector" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="isanet.optimizer.utils.restore_w_to_model">
<code class="sig-prename descclassname">isanet.optimizer.utils.</code><code class="sig-name descname">restore_w_to_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.utils.restore_w_to_model" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="isanet.utils.html" class="btn btn-neutral float-right" title="isanet.utils" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="isanet.neural_network.html" class="btn btn-neutral float-left" title="isanet.neural_network" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Alessandro Cudazzo, Giulia Volpi

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>