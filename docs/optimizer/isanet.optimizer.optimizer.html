

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>isanet.optimizer.optimizer &mdash; IsaNet ML Lib 0.1 beta documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="isanet.optimizer.SGD" href="isanet.optimizer.SGD.html" />
    <link rel="prev" title="isanet.optimizer" href="../isanet.optimizer.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> IsaNet ML Lib
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Package Modules:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../isanet.activation.html">isanet.activation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../isanet.datasets.html">isanet.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../isanet.metrics.html">isanet.metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../isanet.model.html">isanet.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../isanet.model_selection.html">isanet.model_selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../isanet.neural_network.html">isanet.neural_network</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../isanet.optimizer.html">isanet.optimizer</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">isanet.optimizer.optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="isanet.optimizer.SGD.html">isanet.optimizer.SGD</a></li>
<li class="toctree-l2"><a class="reference internal" href="isanet.optimizer.NCG.html">isanet.optimizer.NCG</a></li>
<li class="toctree-l2"><a class="reference internal" href="isanet.optimizer.LBFGS.html">isanet.optimizer.LBFGS</a></li>
<li class="toctree-l2"><a class="reference internal" href="isanet.optimizer.linesearch.html">isanet.optimizer.linesearch</a></li>
<li class="toctree-l2"><a class="reference internal" href="isanet.optimizer.utils.html">isanet.optimizer.utils</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../isanet.utils.html">isanet.utils</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">IsaNet ML Lib</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../isanet.optimizer.html">isanet.optimizer</a> &raquo;</li>
        
      <li>isanet.optimizer.optimizer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/optimizer/isanet.optimizer.optimizer.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-isanet.optimizer.optimizer">
<span id="isanet-optimizer-optimizer"></span><h1>isanet.optimizer.optimizer<a class="headerlink" href="#module-isanet.optimizer.optimizer" title="Permalink to this headline">¶</a></h1>
<p>Optimizer Module.
This module the module provides a basic optimizer class that can be extended
to implement an optimizer class by specifying the operations to be performed
during each step.</p>
<p>The backpropagation method compute the gradient on the following objective
function (Loss)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Loss</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="n">sum_k</span> <span class="p">(</span><span class="n">y_i</span> <span class="o">-</span><span class="n">y_i</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="s1">&#39;)^2</span>
</pre></div>
</div>
<p>and the step method must be implemented.</p>
<p>Moreover it’s provides an EarlyStopping callback that can
be used during the fitting phase a model.</p>
<dl class="py class">
<dt id="isanet.optimizer.optimizer.EarlyStopping">
<em class="property">class </em><code class="sig-prename descclassname">isanet.optimizer.optimizer.</code><code class="sig-name descname">EarlyStopping</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">monitor</span><span class="o">=</span><span class="default_value">'val_loss_mse'</span></em>, <em class="sig-param"><span class="n">eps</span><span class="o">=</span><span class="default_value">1e-13</span></em>, <em class="sig-param"><span class="n">patience</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.EarlyStopping" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Stop training when a the MSE (Mean squared error) on validation
(generalization error) is increasing and exceeds a certain threshold
for a finite number of epochs.</p>
<p class="rubric">Notes</p>
<p>The class define the generalization loss at epoch t to be the relative
increase of the validation error over the minimum-so-far (in percent):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GL</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">mse_val</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">/</span><span class="n">min_mse_val</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Then it will stop the optimization when the generatilization loss exceeds
a certain thresold for a finite number of epochs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">G</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>monitor</strong> (<em>String</em>) – Quantity to be monitored.</p></li>
<li><p><strong>eps</strong> (<em>float  - E.g. 'val_loss_mse'</em>) – Threshold that is used to decide whether to stop the
fitting of the model (it stops if this is true and after
a number of epoch &gt; ‘patience’).</p></li>
<li><p><strong>patience</strong> (<em>integer</em>) – Number of epochs that mse should be worse after which training
will be stopped.</p></li>
<li><p><strong>verbose</strong> (<em>boolean</em><em>, </em><em>default=False</em>) – Whether to print progress messages to stdout.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="isanet.optimizer.optimizer.EarlyStopping.check_early_stop">
<code class="sig-name descname">check_early_stop</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">epoch</span></em>, <em class="sig-param"><span class="n">history</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.EarlyStopping.check_early_stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if Early Stopping criteria has occurred.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>isanet.model.MLP</em>) – </p></li>
<li><p><strong>epoch</strong> (<em>integer</em>) – Current number of epoch (epoch == 0 is the first epoch).</p></li>
<li><p><strong>history</strong> (<em>dict</em>) – Contains, for the current epoch, the values of mse, mee and accuracy
for training and validation and the time taken to compute that epoch.
This parameter is used to check the value of current MSE (Mean squared
error) on validation (generalization error).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>True if the Early stopping has occurred, else False</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>boolean</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.EarlyStopping.get_min_val">
<code class="sig-name descname">get_min_val</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.EarlyStopping.get_min_val" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the min mse on validation if a minimum of the generalization
has been reached after overfitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Min of the mse on validation.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.EarlyStopping.get_weights_backup">
<code class="sig-name descname">get_weights_backup</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.EarlyStopping.get_weights_backup" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the weights’s backup if a minimum of the generalization
has been reached after overfitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Weights’s backup.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list of arrays-like</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="isanet.optimizer.optimizer.Optimizer">
<em class="property">class </em><code class="sig-prename descclassname">isanet.optimizer.optimizer.</code><code class="sig-name descname">Optimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">loss</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">tol</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">n_iter_no_change</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">norm_g_eps</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">l_eps</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">debug</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="simple">
<dt>This class implemets the general optimizer.</dt><dd><p>It must be extended to be used, since method ‘step’ must be implemented.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<em>String</em><em>, </em><em>e.g. 'loss_mse'</em><em> or </em><em>'loss_mse_reg'</em>) – When implement this class, a loss to monitor must be specified: MSE or MLE+REG</p></li>
<li><p><strong>epoch</strong> (<em>integer</em><em>, </em><em>default=0</em>) – Total number of iterations performed by the optimizer.</p></li>
<li><p><strong>model</strong> (<em>isanet.model.MLP</em>) – Specify the Multilayer Perceptron object to optimize</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=None</em>) – Tolerance for the optimization. When the loss on training is
not improving by at least tol for ‘n_iter_no_change’ consecutive
iterations convergence is considered to be reached and training stops.</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>integer</em><em>, </em><em>default=None</em>) – Maximum number of epochs with no improvements &gt; tol.</p></li>
<li><p><strong>norm_g_eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Threshold that is used to decide whether to stop the
fitting of the model (it stops if the norm of the gradient reaches
‘norm_g_eps’).</p></li>
<li><p><strong>l_eps</strong> (<em>float</em><em>, </em><em>optional</em>) – Threshold that is used to decide whether to stop the
fitting of the model (it stops if the loss function reaches
‘l_eps’).</p></li>
<li><p><strong>debug</strong> (<em>boolean</em><em>, </em><em>default=False</em>) – If True, allows you to perform iterations one at a time, pressing the Enter key.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.backpropagation">
<code class="sig-name descname">backpropagation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">weights</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.backpropagation" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the derivative of 1/2 sum_n (y_i -y_i’)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>isanet.model.MLP</em>) – Specify the Multilayer Perceptron object to optimize</p></li>
<li><p><strong>weights</strong> (<em>list</em>) – List of arrays, the ith array represents all the
weights of each neuron in the ith layer.</p></li>
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p></li>
<li><p><strong>Y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_output</em><em>)</em>) – The target values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>contains the gradients for each layer to be used in the delta rule.
Each index in the list represents the ith layer. (from the first
hidden layer to the output layer).:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">E</span><span class="o">.</span><span class="n">g</span><span class="o">.</span> <span class="mi">0</span> <span class="o">-&gt;</span> <span class="n">first</span> <span class="n">hidden</span> <span class="n">layer</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">output</span> <span class="n">layer</span>
<span class="n">where</span> <span class="n">n</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">number</span> <span class="n">of</span> <span class="n">hidden</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">net</span><span class="o">.</span>
</pre></div>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">weights</span></em>, <em class="sig-param"><span class="n">X</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Uses the weights passed to the function to make the Feed-Forward step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weights</strong> (<em>list</em>) – List of arrays, the ith array represents all the
weights of each neuron in the ith layer.</p></li>
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Output of all neurons for input X.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.get_batch">
<code class="sig-name descname">get_batch</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">X_train</span></em>, <em class="sig-param"><span class="n">Y_train</span></em>, <em class="sig-param"><span class="n">batch_size</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.get_batch" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p></li>
<li><p><strong>Y_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_output</em><em>)</em>) – The target values.</p></li>
<li><p><strong>batch_size</strong> (<em>integer</em>) – Size of minibatches for the optimizer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Each key of the dictionary is a integer value from 0 to
number_of_batch -1 and define a batch. Each element is a
dictionary and has two key: ‘batch_x_train’ and ‘batch_y_train’
and refer to the portion of data and target respectively used
for the training.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict of dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.optimize">
<code class="sig-name descname">optimize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">epochs</span></em>, <em class="sig-param"><span class="n">X_train</span></em>, <em class="sig-param"><span class="n">Y_train</span></em>, <em class="sig-param"><span class="n">validation_data</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_size</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">es</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">verbose</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.optimize" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>isanet.model.MLP</em>) – Specify the Multilayer Perceptron object to optimize.</p></li>
<li><p><strong>epochs</strong> (<em>integer</em>) – Maximum number of epochs.</p></li>
<li><p><strong>X_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p></li>
<li><p><strong>Y_train</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_output</em><em>)</em>) – The target values.</p></li>
<li><p><strong>validation_data</strong> (<em>list of arrays-like</em><em>, </em><em>[</em><em>X_val</em><em>, </em><em>Y_val</em><em>]</em><em>, </em><em>optional</em>) – Validation set.</p></li>
<li><p><strong>batch_size</strong> (<em>integer</em><em>, </em><em>optional</em>) – Size of minibatches for the optimizer.
When set to “none”, the optimizer will performe a full batch.</p></li>
<li><p><strong>es</strong> (<em>isanet.callbacks.EarlyStopping</em><em>, </em><em>optional</em>) – When set to None it will only use the <code class="docutils literal notranslate"><span class="pre">epochs</span></code> to finish training.
Otherwise, an EarlyStopping type object has been passed and will stop
training if the model goes overfitting after a number of consecutive iterations.
See docs in optimizier module for the EarlyStopping Class.</p></li>
<li><p><strong>verbose</strong> (<em>integer</em><em>, </em><em>default=0</em>) – Controls the verbosity: the higher, the more messages.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>integer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="isanet.optimizer.optimizer.Optimizer.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">X</span></em>, <em class="sig-param"><span class="n">Y</span></em>, <em class="sig-param"><span class="n">verbose</span></em><span class="sig-paren">)</span><a class="headerlink" href="#isanet.optimizer.optimizer.Optimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>It must be implemented by the derived class (SGD/NCG/LBFGS).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>isanet.model.MLP</em>) – <p>Specify the Multilayer Perceptron object to optimize</p>
<dl class="simple">
<dt>X<span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>The input data.</p>
</dd>
</dl>
</p></li>
<li><p><strong>Y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_output</em><em>)</em>) – The target values.</p></li>
<li><p><strong>verbose</strong> (<em>integer</em><em>, </em><em>default=0</em>) – Controls the verbosity: the higher, the more messages.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>NotImplementedError</strong> – </p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="isanet.optimizer.SGD.html" class="btn btn-neutral float-right" title="isanet.optimizer.SGD" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../isanet.optimizer.html" class="btn btn-neutral float-left" title="isanet.optimizer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Alessandro Cudazzo, Giulia Volpi

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>